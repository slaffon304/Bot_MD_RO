import { Bot, webhookCallback, InlineKeyboard } from "grammy";
import {
  getHistory, pushMessage, clearHistory,
  getUserModel, setUserModel,
  getUserLang, setUserLang, setLangManual, isLangManual,
} from "../lib/store.js";

// â”€â”€ ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const provider = (process.env.PROVIDER || "none").toLowerCase();
const envModel = process.env.MODEL || "";
const FORCE_WEB_FOR_OPEN = (process.env.FORCE_WEB_FOR_OPEN ?? "1") !== "0";
const SOURCE_LIMIT = Math.max(1, Number(process.env.SOURCE_LIMIT || 2));      // Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ 2 Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°
const EXTRACT_CHARS = Math.max(60, Number(process.env.EXTRACT_CHARS || 220)); // Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ 220 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ²Ñ‹Ğ´ĞµÑ€Ğ¶ĞºĞ¸

function defaultModel() { return envModel || "gpt-4o-mini"; }
function isToolCapableModel(m){ return /gpt-4o/i.test(m); }
function isOpenModelNeedingWeb(m){ return /(meta-llama|llama|mistral)/i.test(m); }

function chunkAndReply(ctx, text) {
  const max = 3800, parts = [];
  for (let i = 0; i < text.length; i += max) parts.push(text.slice(i, i + max));
  return parts.reduce((p, t) => p.then(() => ctx.reply(t, { reply_to_message_id: ctx.message.message_id })), Promise.resolve());
}

// â”€â”€ LLM ĞºĞ»Ğ¸ĞµĞ½Ñ‚ (OpenRouter) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function getLLMClient() {
  if (provider !== "openrouter") return null;
  const apiKey = process.env.OPENROUTER_API_KEY || "";
  if (!apiKey) return null;
  const OpenAI = (await import("openai")).default;
  return new OpenAI({ apiKey, baseURL: "https://openrouter.ai/api/v1" });
}

// â”€â”€ Ğ¯Ğ·Ñ‹Ğº â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function detectLangFromTG(code) {
  const s = (code || "").toLowerCase().split("-")[0];
  return ["ru","ro","en"].includes(s) ? s : "en";
}

// Ğ±Ğ°Ğ»Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾â€‘Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ (ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ² Ğº Ğ¾Ğ¿ĞµÑ‡Ğ°Ñ‚ĞºĞ°Ğ¼)
function detectLangFromText(text) {
  if (!text) return null;
  const lower = text.toLowerCase();

  // Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ (Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‡Ğ°ÑÑ‚Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ‡Ğ°Ñ‚ĞºĞ¸)
  const roWords = ["este","sunt","mÃ¢ine","maine","mÃ®ine","azi","astÄƒzi","astazi","vreme","vremea","oraÈ™","bunÄƒ","salut","prognozÄƒ","meteo","moldova","romÃ¢nia","chiÈ™inÄƒu","bucureÈ™ti","bÄƒlÈ›i","balti"];
  const enWords = ["weather","wheather","forecast","tomorrow","tommorow","tomorow","tommorrow","today","hello","hi","city","ny","nyc","new york","what","how"];

  const roDiacritics = (text.match(/[ÄƒÃ¢Ã®È™È›Ä‚Ã‚ÃÈ˜Èš]/g) || []).length;
  const enAsciiLetters = (text.match(/[a-z]/gi) || []).length; // Ğ»Ğ°Ñ‚Ğ¸Ğ½Ğ¸Ñ†Ğ° Ğ² Ñ†ĞµĞ»Ğ¾Ğ¼

  // ÑÑ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ Â«Ğ¾Ñ‡ĞºĞ¸Â»
  let roScore = roDiacritics;
  let enScore = 0;

  // Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼-Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼
  for (const w of roWords) if (lower.includes(w)) roScore += 2;
  for (const w of enWords) if (lower.includes(w)) enScore += 2;

  // ĞµÑĞ»Ğ¸ Ğ»Ğ°Ñ‚Ğ¸Ğ½Ğ¸Ñ†Ñ‹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾, Ñ‡ÑƒÑ‚ÑŒ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ¼ EN
  if (enAsciiLetters > 0) enScore += 1;

  // ĞµÑĞ»Ğ¸ Ğ¾Ğ±Ğ° Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ â€” Ñ€ĞµÑˆĞ°ĞµĞ¼ Ğ¿Ğ¾ Ğ±Ã³Ğ»ÑŒÑˆĞµĞ¼Ñƒ ÑÑ‡Ñ‘Ñ‚Ñƒ;
  // Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ²ĞµĞ½ÑÑ‚Ğ²Ğµ: ĞµÑĞ»Ğ¸ Ğ´Ğ¸Ğ°ĞºÑ€Ğ¸Ñ‚Ğ¸Ğº Ğ¼Ğ°Ğ»Ğ¾ (<=1) Ğ¸ ĞµÑÑ‚ÑŒ ENâ€‘Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ â€” Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ EN
  if (enScore > roScore) return "en";
  if (roScore > enScore) return "ro";
  if (enScore === roScore) {
    if (roDiacritics <= 1 && enWords.some(w => lower.includes(w))) return "en";
    if (roWords.some(w => lower.includes(w))) return "ro";
  }
  return null; // Ğ½Ğµ ÑƒĞ²ĞµÑ€ĞµĞ½Ñ‹ â€” Ğ¿ÑƒÑÑ‚ÑŒ Ñ€ĞµÑˆĞ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¹
}

async function resolveLang(ctx, text) {
  const userId = ctx.from.id;
  const saved = await getUserLang(userId);
  const manual = await isLangManual(userId);
  const tg = detectLangFromTG(ctx.from?.language_code);
  const fromText = detectLangFromText(text);

  if (manual) return saved || tg || "en";
  if (fromText && fromText !== saved) { await setUserLang(userId, fromText); return fromText; }
  if (saved) return saved;
  await setUserLang(userId, tg);
  return tg;
}

function sysPrompt(lang){
  if (lang === "ro") return "EÈ™ti un asistent concis È™i util. RÄƒspunde Ã®n romÃ¢nÄƒ.";
  if (lang === "en") return "You are a concise and helpful assistant. Answer in English.";
  return "Ğ¢Ñ‹ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚. ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ Ğ¿Ğ¾-Ñ€ÑƒÑÑĞºĞ¸.";
}

const NAV = {
  ru: `ĞŸÑ€Ğ¸Ğ²ĞµÑ‚! ğŸ‘‹ Ğ”Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ˜Ğ˜ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½Ğ¾Ğº, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸.
ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹:
â€¢ /model â€” Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (GPTâ€‘4oâ€‘mini, Llama, Mistral)
â€¢ /new â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³
â€¢ /web Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ â€” Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ²ĞµĞ±â€‘Ğ¿Ğ¾Ğ¸ÑĞº
â€¢ /lang ru|ro|en â€” ÑĞ·Ñ‹Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°
â€¢ /help â€” ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹
Ğ¡ĞºĞ¾Ñ€Ğ¾: /img, /video, /tts, /stats`,
  ro: `Salut! ğŸ‘‹ Acces la AI pentru text, imagini, video È™i muzicÄƒ.
Comenzi:
â€¢ /model â€” alege modelul (GPTâ€‘4oâ€‘mini, Llama, Mistral)
â€¢ /new â€” dialog nou
â€¢ /web Ã®ntrebare â€” cÄƒutare web manualÄƒ
â€¢ /lang ru|ro|en â€” limba interfeÈ›ei
â€¢ /help â€” comenzi
Ãn curÃ¢nd: /img, /video, /tts, /stats`,
  en: `Hi! ğŸ‘‹ Access AI for text, images, video, and music.
Commands:
â€¢ /model â€” choose a model (GPTâ€‘4oâ€‘mini, Llama, Mistral)
â€¢ /new â€” new chat
â€¢ /web query â€” manual web search
â€¢ /lang ru|ro|en â€” interface language
â€¢ /help â€” commands
Coming soon: /img, /video, /tts, /stats`
};
const langKB = new InlineKeyboard().text("RU","lang:ru").text("RO","lang:ro").text("EN","lang:en");

// â”€â”€ Web search (Tavily) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function tavilySearch(query, maxResults) {
  const key = process.env.TAVILY_API_KEY || "";
  if (!key) return { ok:false, error:"NO_TAVILY_KEY" };
  const wanted = Math.min(Math.max(maxResults || SOURCE_LIMIT, 1), SOURCE_LIMIT + 1);
  const resp = await fetch("https://api.tavily.com/search", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ api_key:key, query, search_depth:"basic", include_answer:false, time_range:"w", max_results:wanted })
  });
  if (!resp.ok) return { ok:false, error:`HTTP_${resp.status}` };
  const data = await resp.json();
  return { ok:true, data };
}

// â”€â”€ ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Â«ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ/Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ°Â» + Ğ¾Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ¾Ğº â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function rmDiacriticsRo(s) {
  return (s || "")
    .normalize("NFD")
    .replace(/[\u0300-\u036f]/g, "")
    .replace(/[ÄƒÃ¢]/g, "a")
    .replace(/[Ã®]/g, "i")
    .replace(/[È™ÅŸ]/g, "s")
    .replace(/[È›Å£]/g, "t");
}
function levenshtein(a, b) {
  a = a || ""; b = b || "";
  const m = a.length, n = b.length;
  const dp = new Array(n + 1).fill(0).map((_, j) => j);
  for (let i = 1; i <= m; i++) {
    let prev = dp[0];
    dp[0] = i;
    for (let j = 1; j <= n; j++) {
      const temp = dp[j];
      const cost = a[i - 1] === b[j - 1] ? 0 : 1;
      dp[j] = Math.min(dp[j] + 1, dp[j - 1] + 1, prev + cost);
      prev = temp;
    }
  }
  return dp[n];
}
function fuzzyHasToken(token, dict) {
  token = rmDiacriticsRo(token.toLowerCase());
  return dict.some(w => levenshtein(token, rmDiacriticsRo(w)) <= 1);
}
function normalizeTimeAndQuery(text, lang) {
  const tokens = (text || "")
    .toLowerCase()
    .split(/[^a-zÄƒÃ¢Ã®È™È›ÅŸÅ£a-ÑÑ‘0-9]+/i)
    .filter(Boolean);

  const roTomorrow = ["mÃ¢ine","maine","mÃ®ine"], roToday = ["azi","astÄƒzi","astazi"];
  const enTomorrow = ["tomorrow","tmrw","tmr","tommorow","tomorow","tommorrow"], enToday = ["today","2day","td"];
  const ruTomorrow = ["Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ°"], ruToday = ["ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ","ÑĞµĞ¹Ñ‡Ğ°Ñ"];

  let timeframe = null;
  for (const t of tokens) {
    if (fuzzyHasToken(t, roTomorrow) || fuzzyHasToken(t, enTomorrow) || fuzzyHasToken(t, ruTomorrow)) { timeframe = "tomorrow"; break; }
    if (fuzzyHasToken(t, roToday)    || fuzzyHasToken(t, enToday)    || fuzzyHasToken(t, ruToday))    { timeframe = timeframe || "today"; }
  }

  let q = text || "";
  if (timeframe === "tomorrow") {
    if (lang === "ro") q += " mÃ¢ine maine";
    else if (lang === "ru") q += " Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ°";
    else q += " tomorrow";
  } else if (timeframe === "today") {
    if (lang === "ro") q += " azi";
    else if (lang === "ru") q += " ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ";
    else q += " today";
  }
  return { timeframe, corrected: q.trim() };
}

// â”€â”€ Ğ¡ÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² + ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ğ²Ñ‹Ğ´ĞµÑ€Ğ¶ĞºĞ¸) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function summarizeSystem(lang){
  const common = `CiteazÄƒ cel mult ${SOURCE_LIMIT} surse. RespectÄƒ timeframe (azi/today vs mÃ¢ine/tomorrow). Doar fapte din Surse. Liste + referinÈ›e [1], [2]; la final â€” sursele.`;
  if (lang==="ro") return "EÈ™ti un asistent web. RÄƒspunde pe scurt Ã®n romÃ¢nÄƒ. " + common;
  if (lang==="en") return `You are a web assistant. Answer briefly in English. Cite at most ${SOURCE_LIMIT} sources. Respect the timeframe. Use only facts from Sources. Bullets + refs [1], [2]; add sources list at the end.`;
  return `Ğ¢Ñ‹ Ğ²ĞµĞ±â€‘Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚. ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ Ğ¿Ğ¾â€‘Ñ€ÑƒÑÑĞºĞ¸. ĞĞµ Ğ±Ğ¾Ğ»ĞµĞµ ${SOURCE_LIMIT} Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². Ğ¡Ğ¾Ğ±Ğ»ÑĞ´Ğ°Ğ¹ Â«ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ/Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ°Â». Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ñ„Ğ°ĞºÑ‚Ñ‹ Ğ¸Ğ· Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². ĞœĞ°Ñ€ĞºĞµÑ€Ñ‹ + [1], [2]; Ğ² ĞºĞ¾Ğ½Ñ†Ğµ â€” ÑÑÑ‹Ğ»ĞºĞ¸.`;
}
function dedupeAndPick(results) {
  const picked = [], seen = new Set();
  for (const r of results || []) {
    try {
      const host = new URL(r.url).hostname.replace(/^www\./,"");
      if (seen.has(host)) continue;
      seen.add(host);
      picked.push(r);
      if (picked.length >= SOURCE_LIMIT) break;
    } catch {}
  }
  return picked;
}
function shortText(s) {
  const clean = String(s || "").replace(/\s+/g, " ").trim();
  return clean.slice(0, EXTRACT_CHARS);
}
async function summarizeWithSources({ question, searchData, model, lang }) {
  const client = await getLLMClient(); if (!client) throw new Error("NO_LLM");
  const selected = dedupeAndPick(searchData?.results || []);
  if (!selected.length) return lang==="ro" ? "Nu am gÄƒsit rezultate." : lang==="en" ? "No results found." : "ĞĞ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾.";

  const list = selected.map((s,i)=>`${i+1}. ${s.title||s.url} â€” ${s.url}`).join("\n");
  const extracts = selected.map((s,i)=>`[${i+1}] ${shortText(s.content)}`).join("\n\n");

  const r = await client.chat.completions.create({
    model, temperature:0.2, max_tokens:450,
    messages:[
      { role:"system", content: summarizeSystem(lang) },
      { role:"user", content:`Question: ${question}\n\nSources:\n${list}\n\nExtracts:\n${extracts}` }
    ]
  });
  return r.choices?.[0]?.message?.content || (lang==="ro"?"Nu am putut genera rÄƒspuns.":"Couldn't generate an answer.");
}

// â”€â”€ Chat modes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function plainChat({ text, hist, model, lang }) {
  const client = await getLLMClient(); if (!client) throw new Error("NO_LLM");
  const r = await client.chat.completions.create({
    model, temperature:0.6, max_tokens:400,
    messages:[ { role:"system", content: sysPrompt(lang) }, ...hist, { role:"user", content:text } ]
  });
  return r.choices?.[0]?.message?.content || (lang==="ro"?"Niciun rÄƒspuns.":"No answer.");
}
async function chatWithAutoSearch({ text, hist, model, lang }) {
  const client = await getLLMClient(); if (!client) throw new Error("NO_LLM");
  const tools = [{
    type:"function",
    function:{ name:"web_search", description:"Search the web for fresh data",
      parameters:{ type:"object", properties:{ query:{type:"string"}, max_results:{type:"integer",default:SOURCE_LIMIT} }, required:["query"] } }
  }];
  const r1 = await client.chat.completions.create({
    model, temperature:0.6, max_tokens:300,
    messages:[ { role:"system", content: sysPrompt(lang) + " If fresh facts are needed (weather, rates, news etc.), call web_search." }, ...hist, { role:"user", content:text } ],
    tools, tool_choice:"auto"
  });
  const msg1 = r1.choices?.[0]?.message;
  const toolCalls = msg1?.tool_calls || [];
  if (toolCalls.length) {
    let args={}; try{ args = JSON.parse(toolCalls[0].function?.arguments || "{}"); }catch{}
    const q0 = (args.query || text).toString();
    const { corrected } = normalizeTimeAndQuery(q0, lang);
    const sr = await tavilySearch(corrected, SOURCE_LIMIT);
    if (!sr.ok) return sr.error==="NO_TAVILY_KEY" ? (lang==="ro"?"AdaugÄƒ TAVILY_API_KEY Ã®n Vercel.": "Add TAVILY_API_KEY in Vercel.") : `Search failed (${sr.error}).`;
    return await summarizeWithSources({ question:q0, searchData:sr.data, model, lang });
  }
  const plain = msg1?.content?.trim();
  return plain || (lang==="ro"?"ÃncearcÄƒ din nou.":"Try again.");
}

// â”€â”€ ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const MODEL_OPTIONS = [
  { id:"gpt-4o-mini", label:"gpt-4o-mini (smart web tools)" },
  { id:"meta-llama/llama-3.1-70b-instruct", label:"Llama 3.1 70B (budget)" },
  { id:"mistralai/mistral-small", label:"Mistral Small (fast/cheap)" }
];
const KNOWN_CMDS = new Set(["start","help","lang","new","model","web"]);

// â”€â”€ Ğ‘Ğ¾Ñ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
let bot;
function getBot() {
  if (bot) return bot;
  const token = process.env.TELEGRAM_BOT_TOKEN; if (!token) return null;
  const b = new Bot(token);

  // ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ /ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ½Ğµ Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ² LLM
  b.use(async (ctx, next) => {
    if (ctx.message?.text?.startsWith("/")) {
      const m = ctx.message.text.match(/^\/(\w+)/);
      const cmd = (m?.[1] || "").toLowerCase();
      if (cmd && !KNOWN_CMDS.has(cmd)) {
        await ctx.reply("Unknown command. See /help.");
        return;
      }
    }
    await next();
  });

  // /start â€” ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€ ÑĞ·Ñ‹ĞºĞ°, Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ
  b.command("start", async (ctx) => {
    await ctx.reply("Choose language / Alege limba / Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ ÑĞ·Ñ‹Ğº:", { reply_markup: langKB });
  });

  // /help
  b.command("help", async (ctx) => {
    const lang = await resolveLang(ctx, "");
    await ctx.reply(NAV[lang]);
  });

  // /lang â€” Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¸Ğ»Ğ¸ ĞºĞ½Ğ¾Ğ¿ĞºĞ°Ğ¼Ğ¸
  b.command("lang", async (ctx) => {
    const arg = ((ctx.message.text || "").trim().split(/\s+/)[1] || "").toLowerCase();
    if (["ru","ro","en"].includes(arg)) {
      await setUserLang(ctx.from.id, arg);
      await setLangManual(ctx.from.id, true);
      await ctx.reply("OK");
      await ctx.reply(NAV[arg]);
      return;
    }
    await ctx.reply("ru | ro | en", { reply_markup: langKB });
  });
  b.callbackQuery(/^lang:(ru|ro|en)$/, async (ctx) => {
    const v = ctx.match[1];
    await setUserLang(ctx.from.id, v);
    await setLangManual(ctx.from.id, true);
    await ctx.answerCallbackQuery({ text: `Lang: ${v.toUpperCase()}` });
    try { await ctx.editMessageText("âœ“"); } catch {}
    const nav = NAV[v] || NAV.en;
    await ctx.reply(nav);
  });

  // /new
  b.command("new", async (ctx) => {
    await clearHistory(ctx.chat.id);
    await ctx.reply("OK. New chat.");
  });

  // /model
  b.command("model", async (ctx) => {
    const kb = new InlineKeyboard();
    for (const m of MODEL_OPTIONS) kb.text(m.label, `m:${m.id}`).row();
    await ctx.reply("Choose a model:", { reply_markup: kb });
  });
  b.callbackQuery(/m:.+/, async (ctx) => {
    const data = ctx.callbackQuery.data || "";
    const chosen = data.split(":")[1];
    const found = MODEL_OPTIONS.find((m) => m.id === chosen);
    if (!found) { await ctx.answerCallbackQuery({ text: "Unknown model", show_alert: true }); return; }
    await setUserModel(ctx.from.id, found.id);
    await ctx.answerCallbackQuery({ text: `Model: ${found.label}` });
    try { await ctx.editMessageText(`Current model: ${found.label}`); } catch {}
  });

  // /web â€” Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº (Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Â«today/tomorrowÂ» Ğ¸ Ğ¾Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ¾Ğº)
  b.command("web", async (ctx) => {
    const text = ctx.message.text || "";
    const q = text.replace(/^\/web(@\S+)?\s*/i, "").trim();
    const lang = await resolveLang(ctx, q);
    if (!q) { await ctx.reply(lang==="ro"?"Scrie: /web Ã®ntrebarea":"Type: /web your query"); return; }
    await ctx.api.sendChatAction(ctx.chat.id, "typing");

    const userModel = await getUserModel(ctx.from.id); const model = userModel || defaultModel();
    const { corrected } = normalizeTimeAndQuery(q, lang);
    const sr = await tavilySearch(corrected, SOURCE_LIMIT);
    if (!sr.ok) { await ctx.reply(sr.error==="NO_TAVILY_KEY" ? "Add TAVILY_API_KEY in Vercel" : `Search failed (${sr.error}).`); return; }
    const ans = await summarizeWithSources({ question:q, searchData:sr.data, model, lang });
    await chunkAndReply(ctx, ans);
    await pushMessage(ctx.chat.id, { role:"user", content:`/web ${q}` });
    await pushMessage(ctx.chat.id, { role:"assistant", content: ans });
  });

  // ĞĞ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ñ‡Ğ°Ñ‚
  b.on("message:text", async (ctx) => {
    const text = ctx.message.text?.trim() || ""; if (!text) return;
    await ctx.api.sendChatAction(ctx.chat.id, "typing");

    const hist = await getHistory(ctx.chat.id);
    const userModel = await getUserModel(ctx.from.id);
    const model = userModel || defaultModel();
    const lang = await resolveLang(ctx, text);

    try {
      let answer;
      if (FORCE_WEB_FOR_OPEN && isOpenModelNeedingWeb(model)) {
        const { corrected } = normalizeTimeAndQuery(text, lang);
        const sr = await tavilySearch(corrected, SOURCE_LIMIT);
        answer = sr.ok ? await summarizeWithSources({ question:text, searchData:sr.data, model, lang })
                       : await plainChat({ text, hist, model, lang });
      } else if (isToolCapableModel(model)) {
        answer = await chatWithAutoSearch({ text, hist, model, lang });
      } else {
        answer = await plainChat({ text, hist, model, lang });
      }
      await pushMessage(ctx.chat.id, { role:"user", content:text });
      await pushMessage(ctx.chat.id, { role:"assistant", content:answer });
      await chunkAndReply(ctx, answer);
    } catch {
      await ctx.reply(lang==="ro"?"Eroare la procesare. ÃncearcÄƒ din nou.":"Error processing. Try again.");
    }
  });

  bot = b;
  return bot;
}

// â”€â”€ HTTPâ€‘Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export default async function handler(req, res) {
  if (req.method !== "POST") return res.status(200).send("OK");
  const b = getBot(); if (!b) return res.status(200).send("NO_TOKEN");
  const handle = webhookCallback(b, "http");
  try { await handle(req, res); } catch { res.status(200).end(); }
                          }
