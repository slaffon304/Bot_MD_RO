import { Bot, webhookCallback, InlineKeyboard } from "grammy";
import {
  getHistory, pushMessage, clearHistory,
  getUserModel, setUserModel,
  getUserLang, setUserLang, setLangManual, isLangManual,
} from "../lib/store.js";

// â”€â”€ ÐšÐ¾Ð½Ñ„Ð¸Ð³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const provider = (process.env.PROVIDER || "none").toLowerCase();
const envModel = process.env.MODEL || "";
const FORCE_WEB_FOR_OPEN = (process.env.FORCE_WEB_FOR_OPEN ?? "1") !== "0";
const SOURCE_LIMIT = Math.max(1, Number(process.env.SOURCE_LIMIT || 2));   // 2 Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ
const EXTRACT_CHARS = Math.max(60, Number(process.env.EXTRACT_CHARS || 220)); // 220 Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ

function defaultModel() { return envModel || "gpt-4o-mini"; }
function isToolCapableModel(m){ return /gpt-4o/i.test(m); }
function isOpenModelNeedingWeb(m){ return /(meta-llama|llama|mistral)/i.test(m); }

function chunkAndReply(ctx, text) {
  const max = 3800, parts = [];
  for (let i = 0; i < text.length; i += max) parts.push(text.slice(i, i + max));
  return parts.reduce((p, t) => p.then(() => ctx.reply(t, { reply_to_message_id: ctx.message.message_id })), Promise.resolve());
}

// â”€â”€ LLM ÐºÐ»Ð¸ÐµÐ½Ñ‚ (OpenRouter) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function getLLMClient() {
  if (provider !== "openrouter") return null;
  const apiKey = process.env.OPENROUTER_API_KEY || "";
  if (!apiKey) return null;
  const OpenAI = (await import("openai")).default;
  return new OpenAI({ apiKey, baseURL: "https://openrouter.ai/api/v1" });
}

// â”€â”€ Ð¯Ð·Ñ‹Ðº â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function detectLangFromTG(code) {
  const s = (code || "").toLowerCase().split("-")[0];
  return ["ru","ro","en"].includes(s) ? s : "en";
}
function detectLangFromText(text) {
  if (!text) return null;
  const hasCyr = /[\u0400-\u04FF]/.test(text);
  if (hasCyr) return "ru";
  const hasRoDiacritics = /[ÄƒÃ¢Ã®È™È›Ä‚Ã‚ÃŽÈ˜Èš]/.test(text);
  const t = text.toLowerCase();
  const roHints = ["este","sunt","mÃ¢ine","maine","mÃ®ine","azi","astÄƒzi","astazi","vreme","vremea","oraÈ™","salut","bunÄƒ","prognozÄƒ","meteo","moldova","romÃ¢nia","chiÈ™inÄƒu","bucureÈ™ti","bÄƒlÈ›i"];
  const enHints = ["is","are","tomorrow","tommorow","tomorow","tmrw","today","weather","forecast","city","hello","hi"];
  if (hasRoDiacritics || roHints.some(w => t.includes(w))) return "ro";
  if (enHints.some(w => t.includes(w))) return "en";
  return null;
}
async function resolveLang(ctx, text) {
  const userId = ctx.from.id;
  const saved = await getUserLang(userId);
  const manual = await isLangManual(userId);
  const tg = detectLangFromTG(ctx.from?.language_code);
  const fromText = detectLangFromText(text);
  if (manual) return saved || tg || "en";
  if (fromText && fromText !== saved) { await setUserLang(userId, fromText); return fromText; }
  if (saved) return saved;
  await setUserLang(userId, tg);
  return tg;
}

function sysPrompt(lang){
  if (lang === "ro") return "EÈ™ti un asistent concis È™i util. RÄƒspunde Ã®n romÃ¢nÄƒ.";
  if (lang === "en") return "You are a concise and helpful assistant. Answer in English.";
  return "Ð¢Ñ‹ ÐºÑ€Ð°Ñ‚ÐºÐ¸Ð¹ Ð¸ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ð¹ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ð¿Ð¾-Ñ€ÑƒÑÑÐºÐ¸.";
}

const NAV = {
  ru: `ÐŸÑ€Ð¸Ð²ÐµÑ‚! ðŸ‘‹ Ð”Ð¾ÑÑ‚ÑƒÐ¿ Ðº Ð˜Ð˜ Ð´Ð»Ñ Ñ‚ÐµÐºÑÑ‚Ð°, ÐºÐ°Ñ€Ñ‚Ð¸Ð½Ð¾Ðº, Ð²Ð¸Ð´ÐµÐ¾ Ð¸ Ð¼ÑƒÐ·Ñ‹ÐºÐ¸.
ÐšÐ¾Ð¼Ð°Ð½Ð´Ñ‹:
â€¢ /model â€” Ð²Ñ‹Ð±Ñ€Ð°Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ (GPTâ€‘4oâ€‘mini, Llama, Mistral)
â€¢ /new â€” Ð½Ð¾Ð²Ñ‹Ð¹ Ð´Ð¸Ð°Ð»Ð¾Ð³
â€¢ /web Ð·Ð°Ð¿Ñ€Ð¾Ñ â€” Ñ€ÑƒÑ‡Ð½Ð¾Ð¹ Ð²ÐµÐ±â€‘Ð¿Ð¾Ð¸ÑÐº
â€¢ /lang ru|ro|en â€” ÑÐ·Ñ‹Ðº Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹ÑÐ°
â€¢ /help â€” ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹
Ð¡ÐºÐ¾Ñ€Ð¾: /img, /video, /tts, /stats`,
  ro: `Salut! ðŸ‘‹ Acces la AI pentru text, imagini, video È™i muzicÄƒ.
Comenzi:
â€¢ /model â€” alege modelul (GPTâ€‘4oâ€‘mini, Llama, Mistral)
â€¢ /new â€” dialog nou
â€¢ /web Ã®ntrebare â€” cÄƒutare web manualÄƒ
â€¢ /lang ru|ro|en â€” limba interfeÈ›ei
â€¢ /help â€” comenzi
ÃŽn curÃ¢nd: /img, /video, /tts, /stats`,
  en: `Hi! ðŸ‘‹ Access AI for text, images, video, and music.
Commands:
â€¢ /model â€” choose a model (GPTâ€‘4oâ€‘mini, Llama, Mistral)
â€¢ /new â€” new chat
â€¢ /web query â€” manual web search
â€¢ /lang ru|ro|en â€” interface language
â€¢ /help â€” commands
Coming soon: /img, /video, /tts, /stats`
};
const langKB = new InlineKeyboard().text("RU","lang:ru").text("RO","lang:ro").text("EN","lang:en");

// â”€â”€ Web search (Tavily) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function tavilySearch(query, maxResults) {
  const key = process.env.TAVILY_API_KEY || "";
  if (!key) return { ok:false, error:"NO_TAVILY_KEY" };
  const wanted = Math.min(Math.max(maxResults || SOURCE_LIMIT, 1), SOURCE_LIMIT + 1); // Ñ‡ÑƒÑ‚ÑŒ Ð±Ð¾Ð»ÑŒÑˆÐµ Ð»Ð¸Ð¼Ð¸Ñ‚Ð° Ð½Ð° Ð²Ñ…Ð¾Ð´Ðµ
  const resp = await fetch("https://api.tavily.com/search", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    // Ð½ÐµÐ´ÐµÐ»Ñ â€” Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð·Ð°Ñ…Ð²Ð°Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ tomorrow/next day Ð¸ ÑÐ²ÐµÐ¶Ð¸Ðµ Ð½Ð¾Ð²Ð¾ÑÑ‚Ð¸
    body: JSON.stringify({ api_key:key, query, search_depth:"basic", include_answer:false, time_range:"w", max_results:wanted })
  });
  if (!resp.ok) return { ok:false, error:`HTTP_${resp.status}` };
  const data = await resp.json();
  return { ok:true, data };
}

// â”€â”€ ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¾Ð¿ÐµÑ‡Ð°Ñ‚Ð¾Ðº (mÃ®neâ†’mÃ¢ine/tomorowâ†’tomorrow) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function rmDiacriticsRo(s) {
  return (s || "")
    .normalize("NFD")
    .replace(/[\u0300-\u036f]/g, "")
    .replace(/[ÄƒÃ¢]/g, "a")
    .replace(/[Ã®]/g, "i")
    .replace(/[È™ÅŸ]/g, "s")
    .replace(/[È›Å£]/g, "t");
}
function levenshtein(a, b) {
  a = a || ""; b = b || "";
  const m = a.length, n = b.length;
  const dp = new Array(n + 1).fill(0).map((_, j) => j);
  for (let i = 1; i <= m; i++) {
    let prev = dp[0];
    dp[0] = i;
    for (let j = 1; j <= n; j++) {
      const temp = dp[j];
      const cost = a[i - 1] === b[j - 1] ? 0 : 1;
      dp[j] = Math.min(dp[j] + 1, dp[j - 1] + 1, prev + cost);
      prev = temp;
    }
  }
  return dp[n];
}
function fuzzyHasToken(token, dict) {
  token = rmDiacriticsRo(token.toLowerCase());
  return dict.some(w => levenshtein(token, rmDiacriticsRo(w)) <= 1);
}
function normalizeTimeAndQuery(text, lang) {
  const tokens = (text || "")
    .toLowerCase()
    .split(/[^a-zÄƒÃ¢Ã®È™È›ÅŸÅ£a-ÑÑ‘0-9]+/i)
    .filter(Boolean);

  const roTomorrow = ["mÃ¢ine","maine","mÃ®ine"], roToday = ["azi","astÄƒzi","astazi"];
  const enTomorrow = ["tomorrow","tmrw","tmr","tommorow","tomorow","tommorrow"], enToday = ["today","2day","td"];
  const ruTomorrow = ["Ð·Ð°Ð²Ñ‚Ñ€Ð°"], ruToday = ["ÑÐµÐ³Ð¾Ð´Ð½Ñ","ÑÐµÐ¹Ñ‡Ð°Ñ"];

  let timeframe = null;
  for (const t of tokens) {
    if (fuzzyHasToken(t, roTomorrow) || fuzzyHasToken(t, enTomorrow) || fuzzyHasToken(t, ruTomorrow)) { timeframe = "tomorrow"; break; }
    if (fuzzyHasToken(t, roToday)    || fuzzyHasToken(t, enToday)    || fuzzyHasToken(t, ruToday))    { timeframe = timeframe || "today"; }
  }

  let q = text || "";
  if (timeframe === "tomorrow") {
    if (lang === "ro") q += " mÃ¢ine maine";
    else if (lang === "ru") q += " Ð·Ð°Ð²Ñ‚Ñ€Ð°";
    else q += " tomorrow";
  } else if (timeframe === "today") {
    if (lang === "ro") q += " azi";
    else if (lang === "ru") q += " ÑÐµÐ³Ð¾Ð´Ð½Ñ";
    else q += " today";
  }
  return { timeframe, corrected: q.trim() };
}

// â”€â”€ Ð¡ÑƒÐ¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ (Ñ Ð»Ð¸Ð¼Ð¸Ñ‚Ð¾Ð¼ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð² Ð¸ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¼Ð¸ Ð²Ñ‹Ð´ÐµÑ€Ð¶ÐºÐ°Ð¼Ð¸) â”€â”€â”€â”€â”€
function summarizeSystem(lang){
  const common = `CiteazÄƒ cel mult ${SOURCE_LIMIT} surse. RespectÄƒ timeframe (azi/today vs mÃ¢ine/tomorrow). FoloseÈ™te doar fapte din Surse. Liste + referinÈ›e [1], [2]; la final â€” lista surselor.`;
  if (lang==="ro") return "EÈ™ti un asistent web. RÄƒspunde pe scurt Ã®n romÃ¢nÄƒ. " + common;
  if (lang==="en") return `You are a web assistant. Answer briefly in English. Cite at most ${SOURCE_LIMIT} sources. Respect the timeframe. Use only facts from Sources. Bullets + refs [1], [2]; add sources list at the end.`;
  return `Ð¢Ñ‹ Ð²ÐµÐ±â€‘Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ ÐºÑ€Ð°Ñ‚ÐºÐ¾ Ð¿Ð¾â€‘Ñ€ÑƒÑÑÐºÐ¸. Ð£ÐºÐ°Ð¶Ð¸ Ð½Ðµ Ð±Ð¾Ð»ÐµÐµ ${SOURCE_LIMIT} Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð². Ð¡Ð¾Ð±Ð»ÑŽÐ´Ð°Ð¹ Â«ÑÐµÐ³Ð¾Ð´Ð½Ñ/Ð·Ð°Ð²Ñ‚Ñ€Ð°Â». Ð¢Ð¾Ð»ÑŒÐºÐ¾ Ñ„Ð°ÐºÑ‚Ñ‹ Ð¸Ð· Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð². ÐœÐ°Ñ€ÐºÐµÑ€Ñ‹ + ÑÑÑ‹Ð»ÐºÐ¸ [1], [2]; Ð² ÐºÐ¾Ð½Ñ†Ðµ â€” ÑÐ¿Ð¸ÑÐ¾Ðº Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð².`;
}
function dedupeAndPick(results) {
  const picked = [];
  const seen = new Set();
  for (const r of results || []) {
    try {
      const host = new URL(r.url).hostname.replace(/^www\./,"");
      if (seen.has(host)) continue;
      seen.add(host);
      picked.push(r);
      if (picked.length >= SOURCE_LIMIT) break;
    } catch {
      // ÐµÑÐ»Ð¸ URL ÐºÑ€Ð¸Ð²Ð¾Ð¹ â€” Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼
    }
  }
  return picked;
}
function shortText(s) {
  const clean = String(s || "").replace(/\s+/g, " ").trim();
  return clean.slice(0, EXTRACT_CHARS);
}
async function summarizeWithSources({ question, searchData, model, lang }) {
  const client = await getLLMClient(); if (!client) throw new Error("NO_LLM");
  const selected = dedupeAndPick(searchData?.results || []);
  if (!selected.length) return lang==="ro" ? "Nu am gÄƒsit rezultate." : lang==="en" ? "No results found." : "ÐÐ¸Ñ‡ÐµÐ³Ð¾ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾.";

  const list = selected.map((s,i)=>`${i+1}. ${s.title||s.url} â€” ${s.url}`).join("\n");
  const extracts = selected.map((s,i)=>`[${i+1}] ${shortText(s.content)}`).join("\n\n");

  const r = await client.chat.completions.create({
    model, temperature:0.2, max_tokens:450,
    messages:[
      { role:"system", content: summarizeSystem(lang) },
      { role:"user", content:`Question: ${question}\n\nSources:\n${list}\n\nExtracts:\n${extracts}` }
    ]
  });
  return r.choices?.[0]?.message?.content || (lang==="ro"?"Nu am putut genera rÄƒspuns.":"Couldn't generate an answer.");
}

// â”€â”€ Chat modes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function plainChat({ text, hist, model, lang }) {
  const client = await getLLMClient(); if (!client) throw new Error("NO_LLM");
  const r = await client.chat.completions.create({
    model, temperature:0.6, max_tokens:400,
    messages:[ { role:"system", content: sysPrompt(lang) }, ...hist, { role:"user", content:text } ]
  });
  return r.choices?.[0]?.message?.content || (lang==="ro"?"Niciun rÄƒspuns.":"No answer.");
}
async function chatWithAutoSearch({ text, hist, model, lang }) {
  const client = await getLLMClient(); if (!client) throw new Error("NO_LLM");
  const tools = [{
    type:"function",
    function:{ name:"web_search", description:"Search the web for fresh data",
      parameters:{ type:"object", properties:{ query:{type:"string"}, max_results:{type:"integer",default:SOURCE_LIMIT} }, required:["query"] } }
  }];
  const r1 = await client.chat.completions.create({
    model, temperature:0.6, max_tokens:300,
    messages:[ { role:"system", content: sysPrompt(lang) + " If fresh facts are needed (weather, rates, news etc.), call web_search." }, ...hist, { role:"user", content:text } ],
    tools, tool_choice:"auto"
  });
  const msg1 = r1.choices?.[0]?.message;
  const toolCalls = msg1?.tool_calls || [];
  if (toolCalls.length) {
    let args={}; try{ args = JSON.parse(toolCalls[0].function?.arguments || "{}"); }catch{}
    const q0 = (args.query || text).toString();
    const { corrected } = normalizeTimeAndQuery(q0, lang);
    const sr = await tavilySearch(corrected, SOURCE_LIMIT);
    if (!sr.ok) return sr.error==="NO_TAVILY_KEY" ? (lang==="ro"?"AdaugÄƒ TAVILY_API_KEY Ã®n Vercel.": "Add TAVILY_API_KEY in Vercel.") : `Search failed (${sr.error}).`;
    return await summarizeWithSources({ question:q0, searchData:sr.data, model, lang });
  }
  const plain = msg1?.content?.trim();
  return plain || (lang==="ro"?"ÃŽncearcÄƒ din nou.":"Try again.");
}

// â”€â”€ ÐœÐ¾Ð´ÐµÐ»Ð¸ Ð¸ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const MODEL_OPTIONS = [
  { id:"gpt-4o-mini", label:"gpt-4o-mini (smart web tools)" },
  { id:"meta-llama/llama-3.1-70b-instruct", label:"Llama 3.1 70B (budget)" },
  { id:"mistralai/mistral-small", label:"Mistral Small (fast/cheap)" }
];
const KNOWN_CMDS = new Set(["start","help","lang","new","model","web"]);

// â”€â”€ Ð‘Ð¾Ñ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
let bot;
function getBot() {
  if (bot) return bot;
  const token = process.env.TELEGRAM_BOT_TOKEN; if (!token) return null;
  const b = new Bot(token);

  // ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ðµ /ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹ Ð½Ðµ Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð² LLM
  b.use(async (ctx, next) => {
    if (ctx.message?.text?.startsWith("/")) {
      const m = ctx.message.text.match(/^\/(\w+)/);
      const cmd = (m?.[1] || "").toLowerCase();
      if (cmd && !KNOWN_CMDS.has(cmd)) {
        await ctx.reply("Unknown command. See /help.");
        return;
      }
    }
    await next();
  });

  // /start â€” ÑÐ½Ð°Ñ‡Ð°Ð»Ð° Ð²Ñ‹Ð±Ð¾Ñ€ ÑÐ·Ñ‹ÐºÐ°, Ð¿Ð¾Ñ‚Ð¾Ð¼ Ð½Ð°Ð²Ð¸Ð³Ð°Ñ†Ð¸Ñ
  b.command("start", async (ctx) => {
    await ctx.reply("Choose language / Alege limba / Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ ÑÐ·Ñ‹Ðº:", { reply_markup: langKB });
  });

  // /help
  b.command("help", async (ctx) => {
    const lang = await resolveLang(ctx, "");
    await ctx.reply(NAV[lang]);
  });

  // /lang â€” Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð¼ Ð¸Ð»Ð¸ ÐºÐ½Ð¾Ð¿ÐºÐ°Ð¼Ð¸
  b.command("lang", async (ctx) => {
    const arg = ((ctx.message.text || "").trim().split(/\s+/)[1] || "").toLowerCase();
    if (["ru","ro","en"].includes(arg)) {
      await setUserLang(ctx.from.id, arg);
      await setLangManual(ctx.from.id, true);
      await ctx.reply("OK");
      await ctx.reply(NAV[arg]);
      return;
    }
    await ctx.reply("ru | ro | en", { reply_markup: langKB });
  });
  b.callbackQuery(/^lang:(ru|ro|en)$/, async (ctx) => {
    const v = ctx.match[1];
    await setUserLang(ctx.from.id, v);
    await setLangManual(ctx.from.id, true);
    await ctx.answerCallbackQuery({ text: `Lang: ${v.toUpperCase()}` });
    try { await ctx.editMessageText("âœ“"); } catch {}
    const nav = NAV[v] || NAV.en;
    await ctx.reply(nav);
  });

  // /new
  b.command("new", async (ctx) => {
    await clearHistory(ctx.chat.id);
    await ctx.reply("OK. New chat.");
  });

  // /model
  b.command("model", async (ctx) => {
    const kb = new InlineKeyboard();
    for (const m of MODEL_OPTIONS) kb.text(m.label, `m:${m.id}`).row();
    await ctx.reply("Choose a model:", { reply_markup: kb });
  });
  b.callbackQuery(/m:.+/, async (ctx) => {
    const data = ctx.callbackQuery.data || "";
    const chosen = data.split(":")[1];
    const found = MODEL_OPTIONS.find((m) => m.id === chosen);
    if (!found) { await ctx.answerCallbackQuery({ text: "Unknown model", show_alert: true }); return; }
    await setUserModel(ctx.from.id, found.id);
    await ctx.answerCallbackQuery({ text: `Model: ${found.label}` });
    try { await ctx.editMessageText(`Current model: ${found.label}`); } catch {}
  });

  // /web â€” Ñ€ÑƒÑ‡Ð½Ð¾Ð¹ Ð¿Ð¾Ð¸ÑÐº (Ñ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹ Ð¾Ð¿ÐµÑ‡Ð°Ñ‚Ð¾Ðº)
  b.command("web", async (ctx) => {
    const text = ctx.message.text || "";
    const q = text.replace(/^\/web(@\S+)?\s*/i, "").trim();
    const lang = await resolveLang(ctx, q);
    if (!q) { await ctx.reply(lang==="ro"?"Scrie: /web Ã®ntrebarea":"Type: /web your query"); return; }
    await ctx.api.sendChatAction(ctx.chat.id, "typing");

    const userModel = await getUserModel(ctx.from.id); const model = userModel || defaultModel();
    const { corrected } = normalizeTimeAndQuery(q, lang);
    const sr = await tavilySearch(corrected, SOURCE_LIMIT);
    if (!sr.ok) { await ctx.reply(sr.error==="NO_TAVILY_KEY" ? "Add TAVILY_API_KEY in Vercel" : `Search failed (${sr.error}).`); return; }
    const ans = await summarizeWithSources({ question:q, searchData:sr.data, model, lang });
    await chunkAndReply(ctx, ans);
    await pushMessage(ctx.chat.id, { role:"user", content:`/web ${q}` });
    await pushMessage(ctx.chat.id, { role:"assistant", content: ans });
  });

  // ÐžÐ±Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ñ‡Ð°Ñ‚
  b.on("message:text", async (ctx) => {
    const text = ctx.message.text?.trim() || ""; if (!text) return;
    await ctx.api.sendChatAction(ctx.chat.id, "typing");

    const hist = await getHistory(ctx.chat.id);
    const userModel = await getUserModel(ctx.from.id);
    const model = userModel || defaultModel();
    const lang = await resolveLang(ctx, text);

    try {
      let answer;
      if (FORCE_WEB_FOR_OPEN && isOpenModelNeedingWeb(model)) {
        const { corrected } = normalizeTimeAndQuery(text, lang);
        const sr = await tavilySearch(corrected, SOURCE_LIMIT);
        answer = sr.ok ? await summarizeWithSources({ question:text, searchData:sr.data, model, lang })
                       : await plainChat({ text, hist, model, lang });
      } else if (isToolCapableModel(model)) {
        answer = await chatWithAutoSearch({ text, hist, model, lang });
      } else {
        answer = await plainChat({ text, hist, model, lang });
      }
      await pushMessage(ctx.chat.id, { role:"user", content:text });
      await pushMessage(ctx.chat.id, { role:"assistant", content:answer });
      await chunkAndReply(ctx, answer);
    } catch {
      await ctx.reply(lang==="ro"?"Eroare la procesare. ÃŽncearcÄƒ din nou.":"Error processing. Try again.");
    }
  });

  bot = b;
  return bot;
}

// â”€â”€ HTTPâ€‘Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸Ðº â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export default async function handler(req, res) {
  if (req.method !== "POST") return res.status(200).send("OK");
  const b = getBot(); if (!b) return res.status(200).send("NO_TOKEN");
  const handle = webhookCallback(b, "http");
  try { await handle(req, res); } catch { res.status(200).end(); }
}
