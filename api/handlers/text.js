const store = require('../../lib/store');
const content = require('../../content.json');
const { 
    GPT_MODELS, 
    isProKey, 
    getModelForTask, 
    gptKeyboard, 
    premiumMsg, 
    resolvePModelByKey 
} = require('../../lib/models');

const OPENROUTER_API_KEY = process.env.OPENROUTER_API_KEY; 

const FOOTER_MSG = {
  ru: "\n\n‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ\nüîÑ –°–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å: /model | ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏: /settingsbot",
  ro: "\n\n‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ\nüîÑ SchimbƒÉ modelul: /model | ‚öôÔ∏è SetƒÉri: /settingsbot",
  en: "\n\n‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ‚ûñ\nüîÑ Change model: /model | ‚öôÔ∏è Settings: /settingsbot"
};

const ASK_FILE_MSG = {
    ru: "üßê –Ø –≤–∏–∂—É —Ñ–∞–π–ª! –ß—Ç–æ –º–Ω–µ —Å –Ω–∏–º —Å–¥–µ–ª–∞—Ç—å? (–û–ø–∏—Å–∞—Ç—å, —Ä–µ—à–∏—Ç—å –∑–∞–¥–∞—á—É –∏–ª–∏ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç?)",
    ro: "üßê VƒÉd fi»ôierul! Ce dore»ôti sƒÉ fac cu el? (SƒÉ-l descriu, sƒÉ rezolv o problemƒÉ sau sƒÉ traduc text?)",
    en: "üßê I see the file! What should I do with it? (Describe it, solve a problem, or translate text?)"
};

// --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
const DAILY_LIMIT = 10;
const IMAGE_MODEL_ID = 'openai/gpt-5-image-mini';

const FREE_MODEL_IDS = [
    'google/gemini-2.0-flash-exp:free',
    'deepseek/deepseek-chat',
    'deepseek/deepseek-r1',
    'meta-llama/llama-3.2-90b-vision-instruct',
    'mistralai/mistral-7b-instruct:free',
    'google/gemini-2.0-flash-lite-preview-02-05:free'
];

// --- –õ–ò–ú–ò–¢–´ ---
async function checkLimit(userId) {
    if (!store.redis) return true; 
    const today = new Date().toISOString().split('T')[0];
    const key = `usage:${today}:${userId}`;
    let current = await store.redis.get(key);
    current = parseInt(current) || 0;
    return current < DAILY_LIMIT;
}

async function incrementLimit(userId) {
    if (!store.redis) return;
    const today = new Date().toISOString().split('T')[0];
    const key = `usage:${today}:${userId}`;
    await store.redis.incr(key);
    await store.redis.expire(key, 86400); 
    await store.redis.incr(`usage:text:${userId}`); 
}

function getModelNiceName(key, lang = 'ru') {
    const m = GPT_MODELS.find(x => x.key === key);
    if (!m) return key;
    return m.label[lang] || m.label.en || m.key;
}

// --- –ó–ê–ü–†–û–° –ö OPENROUTER ---
async function openRouterRequest(messages, modelId) {
    if (!OPENROUTER_API_KEY) return "NO_KEY";
    try {
        const body = {
            "model": modelId,
            "messages": messages
        };

        const isImageModel = modelId.includes('image') || modelId.includes('dall-e') || modelId.includes('flux');
        
        if (!isImageModel) {
            body.temperature = 0.7; 
        } else {
            // –ü–û–ü–´–¢–ö–ê –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø: –î–æ–±–∞–≤–ª—è–µ–º max_tokens –¥–ª—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π
            body.max_tokens = 2048;
        }

        // –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ó–ê–ü–†–û–°–ê (–î–õ–Ø –û–¢–õ–ê–î–ö–ò)
        console.log(`[AI] Sending request to ${modelId}. Image mode: ${isImageModel}`);
        console.log(`[AI] Request Body: ${JSON.stringify(body)}`);

        const response = await fetch("https://openrouter.ai/api/v1/chat/completions", {
            method: "POST",
            headers: {
                "Authorization": `Bearer ${OPENROUTER_API_KEY}`,
                "HTTP-Referer": process.env.VERCEL_URL || 'https://bot.com',
                "X-Title": 'Telegram Bot',
                "Content-Type": "application/json"
            },
            body: JSON.stringify(body)
        });

        if (!response.ok) {
            const errText = await response.text();
            console.error(`[AI] OpenRouter Error (${response.status}):`, errText);
            // –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –∫–∞–∫ –±–ª–æ–∫ –∫–æ–¥–∞
            return `API ERROR: ${response.status}\n\`\`\`json\n${errText}\n\`\`\``; 
        }
        
        const data = await response.json();
        if (!data.choices || data.choices.length === 0) return "API ERROR: Empty response";
        
        return data.choices[0].message.content;
    } catch (error) {
        console.error("[AI] Fetch Error:", error);
        return `NETWORK ERROR: ${error.message}`;
    }
}

// --- MAIN HANDLER ---
async function handleTextMessage(ctx, textInput) {
    const message = ctx.message;
    const caption = message?.caption || '';
    const text = textInput || caption || ''; 
    const userId = ctx.from.id.toString();

    if (text === '/debug') {
        if (store.getDebugData) {
            const debugInfo = await store.getDebugData(userId);
            await ctx.reply(`üêû DEBUG INFO:\n\n${debugInfo}`);
        } else await ctx.reply('Debug not found.');
        return;
    }

    await ctx.sendChatAction('typing');

    try {
        // 1. –î–∞–Ω–Ω—ã–µ —é–∑–µ—Ä–∞
        let savedModel = 'deepseek'; 
        let savedLang = 'ru';
        let userMode = 'chat';

        try {
            const [m, l, mode] = await Promise.all([
                store.getUserModel(userId),
                store.getUserLang(userId),
                store.getUserMode ? store.getUserMode(userId) : 'chat'
            ]);
            if (m) savedModel = m;
            if (l) savedLang = l;
            if (mode) userMode = mode;
        } catch (e) { console.error("DB Load Error", e); }

        const lang = savedLang;

        // 2. –†–ï–ñ–ò–ú –†–ò–°–û–í–ê–ù–ò–Ø (/image)
        if (userMode === 'image') {
            if (text) {
                const canDraw = await checkLimit(userId);
                if (!canDraw) {
                    const limitMsg = (lang === 'ru') 
                        ? "‚õîÔ∏è **–õ–∏–º–∏—Ç –∏—Å—á–µ—Ä–ø–∞–Ω**\n–í—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ 10 –∑–∞–ø—Ä–æ—Å–æ–≤ —Å–µ–≥–æ–¥–Ω—è."
                        : "‚õîÔ∏è **Daily Limit Reached**";
                    await ctx.reply(limitMsg, { parse_mode: 'Markdown' });
                    return;
                }

                const waitMsg = await ctx.reply("üé® Drawing...");
                
                const prompt = `Generate an image: ${text}`;
                const result = await openRouterRequest([{ role: "user", content: prompt }], IMAGE_MODEL_ID);

                try { await ctx.telegram.deleteMessage(ctx.chat.id, waitMsg.message_id); } catch(e){}

                // –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
                if (!result || result.startsWith("API ERROR") || result.startsWith("NETWORK ERROR")) {
                    await ctx.reply(`‚ö†Ô∏è **Generation Failed**\n\n${result}`, { parse_mode: 'Markdown' });
                    return;
                }

                const urlMatch = result.match(/\((https?:\/\/[^\)]+)\)/) || result.match(/(https?:\/\/[^\s]+)/);
                
                if (urlMatch && urlMatch[1]) {
                    const imageUrl = urlMatch[1];
                    await ctx.replyWithPhoto(imageUrl, { caption: `üñº Generated by GPT-5 Image Mini` });
                    await incrementLimit(userId);
                } else {
                    await ctx.reply(result);
                }
                return; 
            }
        }

        // 3. –û–ë–†–ê–ë–û–¢–ö–ê –§–ê–ô–õ–û–í
        let fileUrl = null;
        let fileType = 'text'; 
        const pendingKey = `pending_file:${userId}`;
        
        const isPhoto = message?.photo;
        const isVoice = message?.voice || message?.audio;
        const isVideo = message?.video || message?.video_note;
        const isDoc = message?.document;

        if (isPhoto || isVoice || isVideo || isDoc) {
             try {
                let fileId = null;
                if (isPhoto) { fileId = message.photo[message.photo.length - 1].file_id; fileType = 'image'; }
                else if (isVoice) { fileId = (message.voice || message.audio).file_id; fileType = 'audio'; }
                else if (isVideo) { fileId = (message.video || message.video_note).file_id; fileType = 'video'; }
                else if (isDoc) { fileId = message.document.file_id; fileType = 'doc'; }

                if (fileId) {
                    const urlObj = await ctx.telegram.getFileLink(fileId);
                    fileUrl = urlObj.href;
                    if (!text) {
                        if (store.redis) await store.redis.set(pendingKey, { url: fileUrl, type: fileType }, { ex: 300 });
                        const askText = ASK_FILE_MSG[lang] || ASK_FILE_MSG.en;
                        await ctx.reply(askText);
                        return;
                    }
                }
            } catch (e) { console.error("File error:", e); }
        } else if (text && store.redis) {
            const pending = await store.redis.get(pendingKey);
            if (pending) { fileUrl = pending.url; fileType = pending.type; await store.redis.del(pendingKey); }
        }

        if (!text && !fileUrl) return;

        // 4. –†–ï–ñ–ò–ú –ß–ê–¢–ê
        let modelToUse = savedModel;
        const pmodel = resolvePModelByKey(modelToUse);
        const realModelId = pmodel || 'deepseek/deepseek-chat';
        
        if (fileType === 'audio') modelToUse = getModelForTask('audio_input');
        else if (fileType === 'video') modelToUse = getModelForTask('video_input');
        else if (fileType === 'doc') modelToUse = getModelForTask('doc_heavy');
        else if (fileType === 'image') {
             if (!pmodel.includes('gpt-4o') && !pmodel.includes('gemini') && !pmodel.includes('claude-3-5')) {
                 modelToUse = 'gemini_flash';
             }
        }

        let isFreeModel = false;
        if (FREE_MODEL_IDS.includes(realModelId) || realModelId.includes(':free')) {
            isFreeModel = true;
        }

        if (!isFreeModel) {
            const canChat = await checkLimit(userId);
            if (!canChat) {
                 const limitMsg = (lang === 'ru') 
                    ? "‚õîÔ∏è **–õ–∏–º–∏—Ç –∏—Å—á–µ—Ä–ø–∞–Ω**\n–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ (DeepSeek, Gemini Flash) —Ä–∞–±–æ—Ç–∞—é—Ç –±–µ–∑–ª–∏–º–∏—Ç–Ω–æ. –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç–µ—Å—å –Ω–∞ –Ω–∏—Ö –≤ /menu."
                    : "‚õîÔ∏è **Daily Limit Reached**\nSwitch to free models in /menu.";
                await ctx.reply(limitMsg, { parse_mode: 'Markdown' });
                return;
            }
        }

        let history = [];
        if (store.getHistory) history = await store.getHistory(userId) || [];

        const niceModelName = getModelNiceName(modelToUse, lang);
        const systemPrompt = {
            role: "system",
            content: `You are a helpful AI assistant running on "${niceModelName}". Reply in the SAME language as the user.`
        };

        let userMessageContent;
        if (fileUrl) {
            userMessageContent = [
                { type: "text", text: text || "Describe this." },
                { type: "image_url", image_url: { url: fileUrl } }
            ];
        } else {
            userMessageContent = text;
        }

        const messagesToSend = [
            systemPrompt,
            ...history, 
            { role: "user", content: userMessageContent }
        ];

        const aiResponse = await openRouterRequest(messagesToSend, realModelId);

        if (!aiResponse || aiResponse.startsWith("API ERROR") || aiResponse.startsWith("NETWORK ERROR")) { 
            await ctx.reply(`‚ö†Ô∏è AI Service Error:\n${aiResponse}`, { parse_mode: 'Markdown' }); 
            return; 
        }

        const footer = FOOTER_MSG[lang] || FOOTER_MSG.en;
        await ctx.reply(aiResponse + footer);

        if (!isFreeModel) {
            await incrementLimit(userId);
        }

        if (store.updateConversation) {
            const historyText = fileUrl ? `[${fileType.toUpperCase()}] ${text}` : text;
            await store.updateConversation(
                userId, 
                { role: "user", content: historyText }, 
                { role: "assistant", content: aiResponse }
            );
        }

    } catch (error) {
        console.error('Handle Text Error:', error);
        await ctx.reply('‚ùå Error.');
    }
}

async function handleClearCommand(ctx) {
    const userId = ctx.from.id.toString();
    if (store.clearHistory) await store.clearHistory(userId);
    await ctx.reply('üóëÔ∏è History cleared.');
}

async function handleModelCommand(ctx) {
    const userId = ctx.from.id.toString();
    let lang = 'ru';
    let model = 'deepseek'; 
    try {
        if (store.getUserLang) lang = await store.getUserLang(userId) || 'ru';
        if (store.getUserModel) model = await store.getUserModel(userId) || 'deepseek';
    } catch(e){}

    const menuText = content.gpt_menu[lang] || content.gpt_menu.en;
    const keyboard = gptKeyboard(lang, model, () => false);
    await ctx.reply(menuText, { parse_mode: 'Markdown', reply_markup: keyboard });
}

async function handleModelCallback(ctx, langCode) {
    const data = ctx.callbackQuery.data;
    const key = data.replace('model_', ''); 
    const userId = ctx.from.id.toString();
    let currentLang = langCode || 'ru';
    try {
        if (!langCode && store.getUserLang) currentLang = await store.getUserLang(userId) || 'ru';
    } catch (e) {}

    if (isProKey(key)) {
        const hasPremium = false; 
        if (!hasPremium) {
            const msg = premiumMsg(currentLang);
            await ctx.answerCbQuery(msg, { show_alert: true });
            return;
        }
    }

    if (store.clearHistory) await store.clearHistory(userId);
    if (store.setUserModel) await store.setUserModel(userId, key);

    try {
        const keyboard = gptKeyboard(currentLang, key, () => false);
        await ctx.editMessageReplyMarkup(keyboard); 
    } catch (e) {}

    const niceName = getModelNiceName(key, currentLang);
    const replyText = (currentLang === 'ru') 
        ? `–í—ã –≤—ã–±—Ä–∞–ª–∏ –º–æ–¥–µ–ª—å ${niceName}. –ò—Å—Ç–æ—Ä–∏—è —Å–±—Ä–æ—à–µ–Ω–∞.` 
        : `You selected model ${niceName}. History reset.`;

    await ctx.reply(replyText + "\n/settingsbot");
    await ctx.answerCbQuery();
}

module.exports = {
    handleTextMessage,
    handleClearCommand,
    handleModelCommand,
    handleModelCallback
};
                
